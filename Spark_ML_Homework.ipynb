{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/manual/spark/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".appName(\"Spark_ML_HW_advertising\") \\\n",
    ".master(\"yarn\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    ".option(\"header\",True) \\\n",
    ".option(\"inferSchema\",True) \\\n",
    ".option(\"sep\",\",\") \\\n",
    ".load(\"file:///home/train/datasets/Advertising.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     TV  Radio  Newspaper  Sales\n",
       "0   1  230.1   37.8       69.2   22.1\n",
       "1   2   44.5   39.3       45.1   10.4\n",
       "2   3   17.2   45.9       69.3    9.3\n",
       "3   4  151.5   41.3       58.5   18.5\n",
       "4   5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- TV: double (nullable = true)\n",
      " |-- Radio: double (nullable = true)\n",
      " |-- Newspaper: double (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, TV: double, Radio: double, Newspaper: double, Sales: double]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# null values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate categorical and numeric null including columns\n",
    "cat_nulls = []\n",
    "\n",
    "num_nulls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_count(df, col_name):\n",
    "    nc = df.filter( \n",
    "        (F.col(col_name).isNull()) | \n",
    "        (F.col(col_name) == \"\") | \n",
    "        (F.col(col_name) == \"NA\")\n",
    "    ).count()\n",
    "    \n",
    "    return nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in df.dtypes:\n",
    "    \n",
    "    nc = null_count(df, col_name[0])\n",
    "    \n",
    "    \n",
    "    if(  nc > 0 ):\n",
    "        print(\"{} {} type has {} null values, % {}\".format(col_name[0], col_name[1], nc, (nc/df_count)))\n",
    "        if col_name[1] == 'string':\n",
    "            cat_nulls.append(col_name[0])\n",
    "        else:\n",
    "            num_nulls.append(col_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(cat_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(num_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no any null column founded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Group columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_cols = []\n",
    "\n",
    "numeric_cols = []\n",
    "\n",
    "discarted_cols = ['ID', 'TV', 'Radio', 'Newspaper']\n",
    "\n",
    "\n",
    "label_col = ['Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler().setHandleInvalid(\"skip\") \\\n",
    ".setInputCols(discarted_cols) \\\n",
    ".setOutputCol('unscaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().setInputCol(\"unscaled_features\").setOutputCol(\"scaled_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = RandomForestRegressor(numTrees=400) \\\n",
    ".setFeaturesCol(\"unscaled_features\") \\\n",
    ".setLabelCol(label_col[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_obj = Pipeline().setStages([assembler, estimator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([.8, .2], seed=142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "print(train_df.count())\n",
    "print(test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+\n",
      "| ID|   TV|Radio|Newspaper|Sales|\n",
      "+---+-----+-----+---------+-----+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|\n",
      "|  5|180.8| 10.8|     58.4| 12.9|\n",
      "|  6|  8.7| 48.9|     75.0|  7.2|\n",
      "+---+-----+-----+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = pipeline_obj.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Sales|prediction        |\n",
      "+-----+------------------+\n",
      "|18.5 |17.715681269671787|\n",
      "|4.8  |7.514044193931963 |\n",
      "|8.6  |9.67198538394973  |\n",
      "|19.0 |18.922156649101318|\n",
      "|9.7  |9.807370970230103 |\n",
      "|18.9 |18.074029811510524|\n",
      "|17.4 |16.202892903674602|\n",
      "|16.6 |15.638277841580125|\n",
      "|10.6 |10.384047162888058|\n",
      "|9.7  |9.705710429069129 |\n",
      "|22.6 |21.304019185388587|\n",
      "|21.2 |20.85319822180858 |\n",
      "|24.2 |23.192859322308692|\n",
      "|11.3 |11.242467359929606|\n",
      "|15.2 |15.941876721288942|\n",
      "|11.5 |11.880594248306684|\n",
      "|11.7 |13.11886633855837 |\n",
      "|14.7 |14.439603525992029|\n",
      "|20.7 |19.741722818849336|\n",
      "|21.8 |20.839537345103672|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.select(label_col[0],'prediction').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+--------------------+------------------+\n",
      "| ID|   TV|Radio|Newspaper|Sales|   unscaled_features|        prediction|\n",
      "+---+-----+-----+---------+-----+--------------------+------------------+\n",
      "|  4|151.5| 41.3|     58.5| 18.5|[4.0,151.5,41.3,5...|17.715681269671787|\n",
      "|  9|  8.6|  2.1|      1.0|  4.8|   [9.0,8.6,2.1,1.0]| 7.514044193931963|\n",
      "| 11| 66.1|  5.8|     24.2|  8.6|[11.0,66.1,5.8,24.2]|  9.67198538394973|\n",
      "| 15|204.1| 32.9|     46.0| 19.0|[15.0,204.1,32.9,...|18.922156649101318|\n",
      "| 25| 62.3| 12.6|     18.3|  9.7|[25.0,62.3,12.6,1...| 9.807370970230103|\n",
      "+---+-----+-----+---------+-----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=label_col[0], metricName='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9135148698784985"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidatorModel, ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(estimator.maxDepth, [5, 8]) \\\n",
    ".addGrid(estimator.maxBins, [30,32,33,35]) \\\n",
    ".addGrid(estimator.numTrees, [15,20,25]) \\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline_obj,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.tuning.CrossValidatorModel"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Param(parent='CrossValidatorModel_6b094fd56be1', name='estimator', doc='estimator to be cross-validated'),\n",
       " Param(parent='CrossValidatorModel_6b094fd56be1', name='estimatorParamMaps', doc='estimator param maps'),\n",
       " Param(parent='CrossValidatorModel_6b094fd56be1', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'),\n",
       " Param(parent='CrossValidatorModel_6b094fd56be1', name='foldCol', doc=\"Param for the column name of user specified fold number. Once this is specified, :py:class:`CrossValidator` won't do random k-fold split. Note that this column should be integer type with range [0, numFolds) and Spark will throw exception on out-of-range fold numbers.\"),\n",
       " Param(parent='CrossValidatorModel_6b094fd56be1', name='numFolds', doc='number of folds for cross validation'),\n",
       " Param(parent='CrossValidatorModel_6b094fd56be1', name='seed', doc='random seed.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328937316552548"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(bestModel.transform(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel.write().overwrite().save(\"file:///home/train/saved_models/pipeline_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access home/train/saved_models/pipeline_model: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l home/train/saved_models/pipeline_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model_loaded = PipelineModel.load(\"file:///home/train/saved_models/pipeline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_pipeline_model = pipeline_model_loaded.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = transformed_pipeline_model.select(*discarted_cols,'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------------------+\n",
      "| ID|   TV|Radio|Newspaper|        prediction|\n",
      "+---+-----+-----+---------+------------------+\n",
      "|  4|151.5| 41.3|     58.5|17.238333333333333|\n",
      "|  9|  8.6|  2.1|      1.0| 6.834166666666666|\n",
      "| 11| 66.1|  5.8|     24.2|10.371085470085466|\n",
      "| 15|204.1| 32.9|     46.0|18.453333333333333|\n",
      "| 25| 62.3| 12.6|     18.3| 9.699146825396824|\n",
      "| 29|248.8| 27.1|     22.9| 19.20052631578947|\n",
      "| 34|265.6| 20.0|      0.3|15.780166666666664|\n",
      "| 41|202.5| 22.3|     31.6|15.969859649122807|\n",
      "| 47| 89.7|  9.9|     35.7| 10.32390293040293|\n",
      "| 50| 66.9| 11.7|     36.8|10.501680708180706|\n",
      "| 53|216.4| 41.7|     39.6|21.993869047619047|\n",
      "| 54|182.6| 46.2|     58.7|19.848499999999994|\n",
      "| 62|261.3| 42.7|     54.7|23.967535714285713|\n",
      "| 83| 75.3| 20.3|     32.5|11.666176470588237|\n",
      "| 86|193.2| 18.4|     65.7|15.633026315789474|\n",
      "| 95|107.4| 14.0|     10.9|12.282176470588235|\n",
      "| 97|197.6|  3.5|      5.9| 12.92797222222222|\n",
      "|104|187.9| 17.2|     17.9|15.123248538011694|\n",
      "|105|238.2| 34.3|      5.3|20.879916666666666|\n",
      "|112|241.7| 38.0|     23.2|21.211999999999996|\n",
      "+---+-----+-----+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df2 = predicted_df.withColumn(\"current_date\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------------------+-----------------------+\n",
      "|ID |TV   |Radio|Newspaper|prediction        |current_date           |\n",
      "+---+-----+-----+---------+------------------+-----------------------+\n",
      "|4  |151.5|41.3 |58.5     |17.238333333333333|2021-10-13 17:35:30.349|\n",
      "|9  |8.6  |2.1  |1.0      |6.834166666666666 |2021-10-13 17:35:30.349|\n",
      "|11 |66.1 |5.8  |24.2     |10.371085470085466|2021-10-13 17:35:30.349|\n",
      "|15 |204.1|32.9 |46.0     |18.453333333333333|2021-10-13 17:35:30.349|\n",
      "|25 |62.3 |12.6 |18.3     |9.699146825396824 |2021-10-13 17:35:30.349|\n",
      "+---+-----+-----+---------+------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_df2.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save predicted table to postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcUrl = \"jdbc:postgresql://localhost/traindb?user=train&password=Ankara06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df2.write.jdbc(url=jdbcUrl,\n",
    "              table=\"advertising_pred\", \n",
    "              mode=\"overwrite\", \n",
    "              properties={\"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>prediction</th>\n",
       "      <th>current_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>17.238333</td>\n",
       "      <td>2021-10-13 17:36:00.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.834167</td>\n",
       "      <td>2021-10-13 17:36:00.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>66.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>24.2</td>\n",
       "      <td>10.371085</td>\n",
       "      <td>2021-10-13 17:36:00.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>204.1</td>\n",
       "      <td>32.9</td>\n",
       "      <td>46.0</td>\n",
       "      <td>18.453333</td>\n",
       "      <td>2021-10-13 17:36:00.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>62.3</td>\n",
       "      <td>12.6</td>\n",
       "      <td>18.3</td>\n",
       "      <td>9.699147</td>\n",
       "      <td>2021-10-13 17:36:00.818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     TV  Radio  Newspaper  prediction            current_date\n",
       "0   4  151.5   41.3       58.5   17.238333 2021-10-13 17:36:00.818\n",
       "1   9    8.6    2.1        1.0    6.834167 2021-10-13 17:36:00.818\n",
       "2  11   66.1    5.8       24.2   10.371085 2021-10-13 17:36:00.818\n",
       "3  15  204.1   32.9       46.0   18.453333 2021-10-13 17:36:00.818\n",
       "4  25   62.3   12.6       18.3    9.699147 2021-10-13 17:36:00.818"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(spark.read\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", jdbcUrl)\n",
    ".option(\"driver\", 'org.postgresql.Driver')\n",
    ".option(\"query\",\"select * from advertising_pred\")\n",
    ".load()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
